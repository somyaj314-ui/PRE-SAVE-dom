# ML Approach for Network Configuration Detection
## Complete Technical Presentation Overview

---

## ğŸ“Š PART 1: FROM RULE-BASED TO ML-BASED DETECTION

### Previous Approach: Rule-Based Detection âŒ
- **Hardcoded Rules**: Each vendor (FortiGate, Palo Alto) had specific patterns
- **Vendor-Specific Code**: Custom detection logic for each system
- **Brittle**: Breaking on small UI changes or new field types
- **Non-Scalable**: Adding new object types = writing new rules for every vendor
- **Example**:
  ```javascript
  // Old approach - rigid, unmaintainable
  if (url.includes('firewall/policy') && objectType === 'policy') {
    // ... custom extraction logic for this specific case
  }
  ```

### New Approach: Machine Learning-Based âœ…
- **Unified Framework**: One system handles all vendors
- **Learned Patterns**: Model learns what fields matter from data
- **Adaptable**: Automatically handles UI changes if trained on new data
- **Scalable**: Add new vendors by collecting training data
- **Intelligent**: Understands relationships between fields and changes

---

## ğŸ§  PART 2: HOW ML ENTERS THE SYSTEM

### The Three-Feature Model Architecture

Your ML system uses **3 independent feature channels** that converge:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UNIFIED ML APPROACH                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    INPUT: Form Submission
                              â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  TEXT FEATURES â”‚      â”‚ STRUCTURAL DATA  â”‚
         â”‚  (Semantic)    â”‚      â”‚   (Field Types)  â”‚
         â”‚                â”‚      â”‚                  â”‚
         â”‚ â€¢ Labels       â”‚      â”‚ â€¢ Nested fields  â”‚
         â”‚ â€¢ Button text  â”‚      â”‚ â€¢ Field count    â”‚
         â”‚ â€¢ Field names  â”‚      â”‚ â€¢ Array presence â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                       â”‚
            TF-IDF Vector              Binary Vector
           (2000 dimensions)           (Flattened DOM)
                  â”‚                       â”‚
                  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                  â”‚    â”‚                  â”‚
                  â–¼    â–¼                  â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   3 Parallel MLPs (128 hidden)  â”‚
            â”‚  Process each feature type      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                  128+128+128 = 384 dims
                       â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Final FC Layer      â”‚
            â”‚  (Classification)     â”‚
            â”‚  Output: 5-20 classes â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  PREDICTION + CONFIDENCE    â”‚
            â”‚  Change Type Classification â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                 DIFF FEATURES
                  (Behavioral)
                       â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Fields Modified  â”‚
              â”‚   (Hashed)       â”‚
              â”‚                  â”‚
              â”‚ Which fields     â”‚
              â”‚ changed in this  â”‚
              â”‚ submission       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                 200-dim Vector
                  (Bit Vector)
                       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚
                                      â–¼ (Concatenate all)
```

---

## ğŸ”„ PART 3: THE COMPLETE DATA PIPELINE

### Stage 1: DATA COLLECTION (Browser Extension)

**Location**: `universal_field_extractor.js` + `ml-unified-collector.js`

```
USER FILLS FORM IN WEB UI
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  UNIVERSAL FIELD EXTRACTOR      â”‚
â”‚  (Content Script)               â”‚
â”‚                                 â”‚
â”‚  â€¢ Captures BEFORE state (on    â”‚
â”‚    page load)                   â”‚
â”‚  â€¢ Captures AFTER state (on     â”‚
â”‚    form save)                   â”‚
â”‚  â€¢ Extracts PURE VALUES         â”‚
â”‚  â€¢ Filters DOM noise            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
        Event: UNIVERSAL_EVENT_SAVED
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ML UNIFIED COLLECTOR           â”‚
â”‚  (Background Script)            â”‚
â”‚                                 â”‚
â”‚  1. Detect Vendor (FortiGate,   â”‚
â”‚     Palo Alto)                  â”‚
â”‚  2. Detect Object Type (policy, â”‚
â”‚     admin_user, interface)      â”‚
â”‚  3. Map Fields to Canonical     â”‚
â”‚     Names (using vendor_field_  â”‚
â”‚     map.json)                   â”‚
â”‚  4. Validate Against Whitelist  â”‚
â”‚  5. Normalize Values            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
     Canonical Sample
     (vendor-agnostic)
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRAINING DATA JSON             â”‚
â”‚  {                              â”‚
â”‚    "metadata": {                â”‚
â”‚      "timestamp": 1702894830,   â”‚
â”‚      "vendor": "fortigate",     â”‚
â”‚      "object_type": "policy"    â”‚
â”‚    },                           â”‚
â”‚    "data": {                    â”‚
â”‚      "before": {...},           â”‚
â”‚      "after": {...}             â”‚
â”‚    },                           â”‚
â”‚    "changes": [...]             â”‚
â”‚  }                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Extraction Features:

#### 1. **TEXT FEATURES** (Semantic Information)
- Visible labels on UI elements
- Button text
- Field names and descriptions
- **Processing**: TF-IDF vectorization (2000 features)
- **Why**: Captures what the human is doing contextually

#### 2. **STRUCTURAL FEATURES** (DOM Topology)
- Flattened binary representation of nested objects
- Field presence/absence indicators
- Array sizes (# of rules, # of addresses, etc.)
- **Processing**: Boolean vector (true/false for each field)
- **Why**: Captures form complexity and organization

#### 3. **DIFF FEATURES** (Behavioral Patterns)
- Which fields were actually modified
- Field interaction patterns
- **Processing**: Hash-based bit vector (200 dimensions)
- **Why**: Captures what the user is changing (not just what's visible)

---

## ğŸ“ˆ STAGE 2: DATA PREPROCESSING

**Script**: `preprocessing.py`

```python
INPUT: data.json (raw training samples)
   â”‚
   â–¼
PREPROCESSING PIPELINE:
   â”‚
   â”œâ”€â–º TEXT PROCESSING
   â”‚   â€¢ Combine visible_labels + button_texts
   â”‚   â€¢ Lowercase & tokenize
   â”‚   â€¢ TF-IDF vectorization â†’ 2000-dim vector
   â”‚
   â”œâ”€â–º STRUCTURAL PROCESSING
   â”‚   â€¢ Flatten nested DOM objects
   â”‚   â€¢ Convert to binary (present=1, absent=0)
   â”‚   â€¢ Pad all to same length
   â”‚
   â””â”€â–º DIFF PROCESSING
       â€¢ Hash field names
       â€¢ Create 200-dim bit vector
       â€¢ 1 if field was modified, 0 otherwise

OUTPUT: train.pkl (PyTorch-compatible tensors)
   â”œâ”€ texts: [N, 2000] TF-IDF vectors
   â”œâ”€ structs: [N, max_struct_len] binary vectors
   â”œâ”€ diffs: [N, 200] bit vectors
   â””â”€ labels: [N] class labels (POLICY_CREATED, INTERFACE_MODIFIED, etc.)
```

---

## ğŸ¤– STAGE 3: MODEL TRAINING

**Script**: `train.py`

### Neural Network Architecture:

```
MODEL STRUCTURE: Multi-Head MLP Fusion
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Input Layer (Concatenated Features):
   â€¢ text_vector: 2000 dims
   â€¢ struct_vector: ~500-1000 dims  
   â€¢ diff_vector: 200 dims
   
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                     â”‚                 â”‚              â”‚
   â–¼                     â–¼                 â–¼              â–¼
 [2000]               [~750]              [200]        
   â”‚                     â”‚                 â”‚              
   â”‚                     â”‚                 â”‚              
   MLP_1                MLP_2              MLP_3         
   (Text)              (Struct)           (Diff)         
   â”‚                     â”‚                 â”‚              
   â”œâ”€ Linear(2000â†’128)   â”‚                 â”‚              
   â”œâ”€ ReLU               â”‚                 â”‚              
   â”œâ”€ Linear(128â†’128)    â”‚                 â”‚              
   â”‚                     â”‚                 â”‚              
   â”‚               â”œâ”€ Linear(~750â†’128)  â”œâ”€ Linear(200â†’128)
   â”‚               â”œâ”€ ReLU              â”œâ”€ ReLU
   â”‚               â”œâ”€ Linear(128â†’128)   â”œâ”€ Linear(128â†’128)
   â”‚                     â”‚                 â”‚
   â–¼                     â–¼                 â–¼
 [128]                 [128]              [128]
 
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         Concatenate: [128+128+128] = [384]
                         â”‚
                         â–¼
                    FC Layer
                Linear(384 â†’ num_classes)
                         â”‚
                         â–¼
                  [5-20] logits
                         â”‚
                         â–¼
                 SoftMax Activation
                         â”‚
                         â–¼
          [5-20] probability scores
```

### Training Parameters:
- **Optimizer**: Adam (lr=1e-3)
- **Loss Function**: Cross-Entropy Loss
- **Batch Size**: 4
- **Epochs**: 25
- **Total Parameters**: ~250K

### Training Process:

```python
For each epoch (25 total):
   For each batch of 4 samples:
      1. Text â†’ text_mlp â†’ 128-dim feature
      2. Struct â†’ struct_mlp â†’ 128-dim feature
      3. Diff â†’ diff_mlp â†’ 128-dim feature
      
      4. Concatenate 3 features â†’ 384-dim vector
      5. Pass through FC layer â†’ logits
      6. SoftMax â†’ probabilities
      
      7. Calculate CrossEntropy Loss
      8. Backprop & Update Weights
      
   Print epoch loss
   
Save: model_artifacts.pkl
   â”œâ”€ model.state_dict() (all weights & biases)
   â”œâ”€ vectorizer (TF-IDF for inference)
   â”œâ”€ label_map (class name to index)
   â””â”€ dims metadata
```

---

## ğŸ”Œ STAGE 4: MODEL DEPLOYMENT

**Script**: `export_model.py`

### Export Process:
```
model_artifacts.pkl (PyTorch binary)
   â”‚
   â”œâ”€â–º Extract TF-IDF vocab & IDF weights
   â”œâ”€â–º Extract all model layer weights & biases
   â”œâ”€â–º Convert to Python lists (JSON-serializable)
   â””â”€â–º Create metadata (labels, dimensions)
   
   â”‚
   â–¼
model_data.json (2-5 MB JSON file)
```

### JSON Structure:
```json
{
  "tfidf": {
    "vocab": ["word1", "word2", ...],
    "idf": [1.2, 0.8, ...]
  },
  "model": {
    "txt_mlp": {
      "l1_weight": [[...], [...], ...],
      "l1_bias": [...],
      "l2_weight": [[...], [...], ...],
      "l2_bias": [...]
    },
    "struct_mlp": {...},
    "diff_mlp": {...},
    "fc": {
      "weight": [[...], [...], ...],
      "bias": [...]
    }
  },
  "metadata": {
    "labels": {0: "POLICY_CREATED", 1: "INTERFACE_MODIFIED", ...},
    "struct_dim": 847,
    "diff_dim": 200
  }
}
```

---

## ğŸ¯ STAGE 5: REAL-TIME INFERENCE

**Script**: `ml-inference.js` (Browser-based)

### Inference Pipeline:

```
NEW FORM SUBMISSION
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ML INFERENCE ENGINE              â”‚
â”‚ (Browser JavaScript)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â”œâ”€â–º 1. PREPROCESS TEXT
   â”‚       â€¢ Extract visible labels
   â”‚       â€¢ Tokenize (remove stopwords)
   â”‚       â€¢ Lookup in vocab
   â”‚       â€¢ Compute TF-IDF
   â”‚       â€¢ L2 normalize
   â”‚       â†’ [2000] float vector
   â”‚
   â”œâ”€â–º 2. PREPROCESS STRUCTURE
   â”‚       â€¢ Flatten DOM object
   â”‚       â€¢ Convert to binary
   â”‚       â€¢ Pad to expected length
   â”‚       â†’ [~750] binary vector
   â”‚
   â”œâ”€â–º 3. PREPROCESS DIFF
   â”‚       â€¢ Extract modified field names
   â”‚       â€¢ Hash each field name
   â”‚       â€¢ Create bit vector
   â”‚       â†’ [200] binary vector
   â”‚
   â”œâ”€â–º 4. FORWARD PASS (MLPs)
   â”‚       â€¢ text_mlp([2000]) â†’ [128]
   â”‚       â€¢ struct_mlp([~750]) â†’ [128]
   â”‚       â€¢ diff_mlp([200]) â†’ [128]
   â”‚
   â”œâ”€â–º 5. CONCATENATE
   â”‚       [128 + 128 + 128] â†’ [384]
   â”‚
   â”œâ”€â–º 6. FINAL CLASSIFICATION
   â”‚       â€¢ fc([384]) â†’ [num_classes]
   â”‚       â€¢ softmax() â†’ probabilities
   â”‚
   â””â”€â–º 7. OUTPUT PREDICTION
       {
         "label": "POLICY_CREATED",
         "confidence": 0.92,
         "probabilities": [0.92, 0.03, 0.02, 0.02, 0.01]
       }
```

---

## ğŸ”‘ KEY TECHNICAL ADVANTAGES

| Aspect | Rule-Based | ML-Based |
|--------|-----------|----------|
| **Scalability** | Linear growth with vendors | Logarithmic (all vendors â†’ one model) |
| **Robustness** | Brittle to UI changes | Learns patterns, handles variations |
| **Accuracy** | Fixed, can miss edge cases | Improves with more data |
| **Maintenance** | High (new rules per vendor) | Low (retrain on new data) |
| **Adaptability** | Manual updates needed | Automatic with new training data |
| **Inference Speed** | Fast (regex matching) | Fast (matrix operations) |
| **Explainability** | Clear rules | Feature importance from attention |

---

## ğŸ“Š DATA COLLECTION STRATEGY

### What We Collect:

1. **Before State**: Form fields when page loads
2. **After State**: Form fields when saved
3. **Changes**: Which fields were modified
4. **Metadata**: 
   - Timestamp
   - Vendor (FortiGate, Palo Alto)
   - Object type (Policy, Interface, Admin User)
   - User ID (if available)

### Data Quality Measures:

```javascript
// STRICT FILTERING
â”œâ”€â–º Remove DOM noise (IDs, Angular internals)
â”œâ”€â–º Map vendor-specific names to canonical forms
â”œâ”€â–º Validate against whitelist (vendor_field_map.json)
â”œâ”€â–º Normalize boolean/numeric values
â””â”€â–º Reject malformed samples
```

### How Much Data Do We Need?

For 5-10 object types Ã— 2 vendors:
- **Minimum**: 500-1000 samples per class
- **Good**: 2000-5000 samples per class
- **Excellent**: 10,000+ samples per class

**For Your Project**:
- Policy Create/Edit: ~1000 samples
- Interface Modify: ~500 samples
- Admin User Create: ~500 samples
- etc.

---

## ğŸ“ SAMPLE TRAINING WORKFLOW

```
DAY 1: Collect Data
   â””â”€ Use browser extension to capture 2000+ real form submissions
   
DAY 2: Preprocess
   â””â”€ preprocessing.py â†’ train.pkl (cleaned, vectorized)
   
DAY 3: Train Model
   â””â”€ train.py â†’ 25 epochs, ~30 seconds (on CPU)
   â””â”€ Saves: model_artifacts.pkl
   
DAY 4: Export & Deploy
   â””â”€ export_model.py â†’ model_data.json
   â””â”€ Load in ml-inference.js
   
DAY 5: Monitor & Iterate
   â””â”€ Track inference accuracy
   â””â”€ Collect more edge cases
   â””â”€ Retrain with larger dataset
```

---

## ğŸ—ï¸ SYSTEM ARCHITECTURE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           BROWSER EXTENSION (Content Script)        â”‚
â”‚                                                     â”‚
â”‚  universal_field_extractor.js                       â”‚
â”‚  â”œâ”€ Captures form state (BEFORE/AFTER)              â”‚
â”‚  â””â”€ Emits UNIVERSAL_EVENT_SAVED                     â”‚
â”‚                                                     â”‚
â”‚           â†“ postMessage                             â”‚
â”‚                                                     â”‚
â”‚  ml-unified-collector.js                            â”‚
â”‚  â”œâ”€ Receives raw event                              â”‚
â”‚  â”œâ”€ Maps to canonical schema                        â”‚
â”‚  â”œâ”€ Validates against whitelist                     â”‚
â”‚  â””â”€ Stores training samples                         â”‚
â”‚       â†“ Download via Ctrl+Shift+D                   â”‚
â”‚       JSON file (training data)                     â”‚
â”‚                                                     â”‚
â”‚  ml-inference.js                                    â”‚
â”‚  â”œâ”€ Loads model_data.json                           â”‚
â”‚  â”œâ”€ Runs inference on new submissions               â”‚
â”‚  â””â”€ Returns prediction + confidence                 â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        â†“ Upload
                        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           BACKEND (Python)                          â”‚
â”‚                                                     â”‚
â”‚  preprocessing.py                                   â”‚
â”‚  â””â”€ Convert JSON â†’ train.pkl (tensors)              â”‚
â”‚                                                     â”‚
â”‚  train.py                                           â”‚
â”‚  â””â”€ Train PyTorch model                             â”‚
â”‚       Saves: model_artifacts.pkl                    â”‚
â”‚                                                     â”‚
â”‚  export_model.py                                    â”‚
â”‚  â””â”€ Convert PyTorch â†’ JSON                          â”‚
â”‚       Exports: model_data.json                      â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        â†“ Deploy
                        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  model_data.json (deployed to extension)            â”‚
â”‚  â””â”€ Loaded by ml-inference.js                       â”‚
â”‚     â”œâ”€ Real-time predictions                        â”‚
â”‚     â””â”€ ~50ms inference latency                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ HOW THE MODEL LEARNS

### Example 1: Learning "Policy Creation"

**Training Samples** (simplified):
```
Sample 1:
  BEFORE: {} (empty)
  AFTER: {policy_name: "allow_http", src_ip: "0.0.0.0/0", ...}
  CHANGES: 8 fields modified
  LABEL: "POLICY_CREATED"

Sample 2:
  BEFORE: {} (empty)
  AFTER: {policy_name: "block_smtp", src_ip: "10.0.0.0/8", ...}
  CHANGES: 7 fields modified
  LABEL: "POLICY_CREATED"

Sample 3:
  BEFORE: {policy_name: "allow_http", ...}
  AFTER: {policy_name: "allow_http", dst_port: "443", ...}
  CHANGES: 1 field modified
  LABEL: "POLICY_MODIFIED"
```

**What the Model Learns**:
- "When BEFORE is empty â†’ likely CREATION"
- "When many fields change together â†’ CREATION"
- "When few fields change â†’ MODIFICATION"
- "Policy names matter (text feature)"
- "Field count matters (struct feature)"

### Example 2: Handling Vendor Variations

**FortiGate Policy**:
```json
{
  "policyid": 1,
  "srcintf": "port1",
  "dstintf": "port2",
  "policy_name": "Policy 1"
}
```

**Palo Alto Policy**:
```json
{
  "name": "Policy 1",
  "from": "untrust",
  "to": "trust",
  "source": ["any"]
}
```

**What ML Does**:
1. Both mapped to canonical schema
2. Both vectorized the same way
3. Model learns universal patterns
4. Works across vendors!

---

## ğŸ¯ REAL-WORLD BENEFITS

1. **Adaptability**: Company X adds new appliance? Just collect data, retrain.
2. **Accuracy**: Model sees patterns humans miss
3. **Speed**: Inference runs in-browser, instant feedback
4. **Compliance**: Track every change with ML confidence score
5. **Analytics**: Identify common change patterns, risks

---

## ğŸ“‹ PERFORMANCE METRICS TO TRACK

During deployment, monitor:

```
INFERENCE METRICS:
â”œâ”€ Latency: < 50ms per prediction âœ“
â”œâ”€ Memory: < 5MB for model JSON
â””â”€ Accuracy: Precision/Recall per class

TRAINING METRICS:
â”œâ”€ Validation loss: Should decrease each epoch
â”œâ”€ Class balance: Each type has enough samples
â””â”€ Feature importance: Which features matter most?
```

---

## ğŸš€ NEXT STEPS (Roadmap)

**Phase 1 (Now)**: Data collection & model training
**Phase 2**: Deploy & monitor in browser
**Phase 3**: Collect edge cases, retrain monthly
**Phase 4**: Add confidence-based filtering
**Phase 5**: Multi-label classification (multiple change types)

---

## ğŸ“ PRESENTATION TALKING POINTS

1. **Opening**: "We're replacing manual rules with a learning system"
2. **Problem**: Rule-based detection doesn't scale across vendors
3. **Solution**: Train a neural network to learn patterns from data
4. **Technical**: Show the 3-feature architecture
5. **Data**: Explain what we collect and how it's cleaned
6. **Model**: Walk through training process
7. **Deployment**: Show real-time inference
8. **Results**: Accuracy improvement vs rule-based
9. **ROI**: Maintenance reduction, faster vendor onboarding
10. **Roadmap**: Future enhancements

---

## ğŸ”¬ TECHNICAL DEEP DIVES (For Q&A)

### Q: Why 3 MLPs instead of 1 big one?
**A**: Feature fusion architecture. Each MLP specializes in one modality:
- Text MLP: semantic understanding
- Struct MLP: form complexity
- Diff MLP: user behavior
Fusion at MLP output allows model to learn cross-feature relationships.

### Q: Why TF-IDF not embeddings?
**A**: TF-IDF is lightweight (runs in browser), interpretable, and sufficient for field name classification. Could upgrade to BERT if needed.

### Q: Why not use RNNs/Transformers?
**A**: Sequential data not needed here. Changes are single-step, not time-series. MLPs sufficient and faster.

### Q: How do we handle new vendors?
**A**: Collect ~500 samples from new vendor, retrain entire model. Canonical schema ensures compatibility.

### Q: Can we do transfer learning?
**A**: Yes! Retrain only final FC layer on new vendor while keeping MLPs frozen. 10x faster training.

