# ğŸ¤ TRAINING & INFERENCE: PRESENTATION SLIDES

## SLIDE 1: Training vs Inference (What's the Difference?)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TRAINING                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Learning Phase (Happens ONCE on Backend Server)       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                                                         â”‚
â”‚  Input: 1000+ labeled examples                         â”‚
â”‚    â”œâ”€ "policy allow http port1 port2" â†’ CREATED       â”‚
â”‚    â”œâ”€ "policy allow dns port3 port4" â†’ MODIFIED       â”‚
â”‚    â””â”€ ...                                              â”‚
â”‚                                                         â”‚
â”‚  Process: 25 iterations (epochs)                       â”‚
â”‚    â”œâ”€ Each time: see all 1000 samples                 â”‚
â”‚    â”œâ”€ Adjust 440K internal weights                    â”‚
â”‚    â””â”€ Get slightly better at predictions              â”‚
â”‚                                                         â”‚
â”‚  Output: Trained model (weights saved)                â”‚
â”‚    â””â”€ "Policy with empty 'before' = likely CREATED"   â”‚
â”‚    â””â”€ "Few fields changed = likely MODIFIED"          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INFERENCE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Prediction Phase (Happens MANY TIMES in Browser)      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                         â”‚
â”‚  Input: 1 new config change (no label yet)             â”‚
â”‚    â”œâ”€ User saves policy with 3 fields changed         â”‚
â”‚    â””â”€ "policy allow https port1 port3 status" (?)     â”‚
â”‚                                                         â”‚
â”‚  Process: Use trained weights (read-only)              â”‚
â”‚    â”œâ”€ ~50ms of matrix multiplications                 â”‚
â”‚    â”œâ”€ No weight updates!                              â”‚
â”‚    â””â”€ Just forward pass through network               â”‚
â”‚                                                         â”‚
â”‚  Output: Prediction + Confidence                      â”‚
â”‚    â””â”€ "POLICY_MODIFIED with 78% confidence"           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

KEY DIFFERENCE:
  Training = LEARNING (weights change)
  Inference = USING (weights fixed)
```

---

## SLIDE 2: Training Loop Visualization

```
THE TRAINING PROCESS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EPOCH 1:     Loss: 47.32  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
EPOCH 2:     Loss: 42.15  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
EPOCH 3:     Loss: 38.87  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
EPOCH 4:     Loss: 35.22  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
EPOCH 5:     Loss: 31.65  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
...
EPOCH 20:    Loss: 8.45   â–ˆâ–ˆ
EPOCH 21:    Loss: 7.98   â–ˆâ–ˆ
EPOCH 22:    Loss: 7.65   â–ˆ
EPOCH 23:    Loss: 7.43   â–ˆ
EPOCH 24:    Loss: 7.28   â–ˆ
EPOCH 25:    Loss: 7.15   â–ˆ

Loss dropped 85% â†’ Model learned!


INSIDE ONE EPOCH (25K samples = 250 batches)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Initialize weights randomly

FOR BATCH 1-250:
  1ï¸âƒ£  PREDICT: 4 samples through network â†’ 4 predictions
  2ï¸âƒ£  COMPARE: predictions vs true labels â†’ loss
  3ï¸âƒ£  ANALYZE: which weights caused errors â†’ gradients
  4ï¸âƒ£  UPDATE: adjust weights slightly â†’ better next time

After epoch: All weights adjusted slightly
After 25 epochs: All weights perfectly tuned


THE "AHA" MOMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

After epoch 1:  Weights: Random
                Model: Guesses randomly (loss=47)

After epoch 10: Weights: Starting to see patterns
                Model: Getting decent (loss=18)

After epoch 25: Weights: Learned relationships
                Model: Very accurate (loss=7)

Now model "understands":
  "If 'before' is empty â†’ probably CREATE"
  "If 1-2 fields changed â†’ probably MODIFY"
  "If field names look like interface â†’ probably INTERFACE_*"
```

---

## SLIDE 3: The Neural Network Brain

```
HOW THE MODEL WORKS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Think of it as a 3-expert voting system:

   TEXT EXPERT              STRUCT EXPERT        DIFF EXPERT
   (reads labels)           (sees form shape)    (watches changes)
        â”‚                          â”‚                    â”‚
        â”‚                          â”‚                    â”‚
        â–¼                          â–¼                    â–¼
   "This mentions             "The form has        "3 fields
    'allow' and               9 fields, looks      changed,
    'https', so               like policy"         that's typical
    probably                                       for create"
    a policy
    policy"
        â”‚                          â”‚                    â”‚
        â”‚                          â”‚                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                 COMBINE VOTES
                 (Weighted Average)
                       â”‚
                       â–¼
                   DECISION
           "This is POLICY_CREATED"
               (with 87% confidence)


THE TECHNICAL VIEW:

Input: 3 types of features
  â”œâ”€ Text: TF-IDF vectors [2000 dims]
  â”‚         (word importance)
  â”œâ”€ Struct: Form shape [847 dims]
  â”‚         (which fields present)
  â””â”€ Diff: Change pattern [200 dims]
           (which fields modified)

Each goes through its own "expert" (MLP):
  â”œâ”€ Text Expert: 2000 â†’ 128 dims
  â”œâ”€ Struct Expert: 847 â†’ 128 dims
  â””â”€ Diff Expert: 200 â†’ 128 dims

Combined knowledge: 128+128+128 = 384 dims

Final judge: 384 â†’ 5 classes (CREATED, MODIFIED, etc)

Softmax: Converts to probabilities (sum to 100%)
```

---

## SLIDE 4: One Prediction Step-by-Step

```
LIVE PREDICTION (What happens when user saves a policy)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

USER SAVES CONFIG
â”‚
â”œâ”€ Form data captured:
â”‚  â”œâ”€ Before: {} (empty)
â”‚  â””â”€ After: {name, srcintf, dstintf, action, ...}
â”‚
â””â”€ Changes: [{field: "name", old: "", new: "Allow DNS"}, ...]


STEP 1: TEXT FEATURE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Words in form: "policy", "allow", "dns", "port1", "port2"

TF-IDF vector: Count how important each word is
  "policy" appears in 50% of samples â†’ weight=0.8
  "allow" appears in 90% of samples â†’ weight=0.3
  "dns" appears in 20% of samples â†’ weight=1.2
  ...
  
Result: [0.12, 0.0, 0.45, ..., 0.02] (2000 dimensions)
         â–²
         â””â”€ Only 10-50 non-zero values


STEP 2: STRUCT FEATURE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Which fields exist?
  name: YES â†’ 1
  srcintf: YES â†’ 1
  dstintf: YES â†’ 1
  action: YES â†’ 1
  logging: NO â†’ 0
  rate_limit: NO â†’ 0
  ...

Result: [1, 1, 1, 1, 0, 0, ..., 1] (847 dimensions)
        â–²
        â””â”€ All either 0 or 1


STEP 3: DIFF FEATURE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
What fields changed?
  name: changed
  srcintf: changed
  dstintf: changed
  action: NOT changed
  ...

Hash changed field names:
  hash("name") % 200 = 45
  hash("srcintf") % 200 = 78
  hash("dstintf") % 200 = 21
  
Result: [0, 0, ..., 1(pos 21), ..., 1(pos 45), ..., 1(pos 78), ..., 0]
        â–²
        â””â”€ Sparse: only 3 out of 200 are 1


STEP 4: NEURAL NETWORK FORWARD PASS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input Features:
  text[2000] â†’ txt_mlp â†’ [128]
  struct[847] â†’ struct_mlp â†’ [128]
  diff[200] â†’ diff_mlp â†’ [128]

Combine:
  [128 + 128 + 128] = [384]
  
Classify:
  [384] â†’ linear layer â†’ [5] logits
  
  logits = [2.3, -1.5, 0.8, 0.2, -0.3]
           â–²
           â””â”€ Raw scores (not probabilities yet)


STEP 5: SOFTMAX CONVERSION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logits [2.3, -1.5, 0.8, 0.2, -0.3]
  â”‚
  â”œâ”€ Exponentiate: e^2.3=9.97, e^-1.5=0.22, ...
  â”‚
  â””â”€ Normalize: divide by sum
  
Result probabilities:
  [0.693, 0.015, 0.155, 0.085, 0.051]
   69.3%   1.5%   15.5%   8.5%   5.1%   â† Sum = 100%


STEP 6: FINAL ANSWER
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Highest probability: 0.693 (69.3%) at position 0

Label mapping:
  0 â†’ "POLICY_CREATED"
  1 â†’ "POLICY_MODIFIED"
  2 â†’ "INTERFACE_CREATED"
  3 â†’ "ADMIN_USER_CREATED"
  4 â†’ "OTHER"

PREDICTION:
{
  "label": "POLICY_CREATED",
  "confidence": 0.693,
  "reason": "Empty 'before' state + multiple fields changed"
}

â±ï¸ Time: 45-50 milliseconds


SEND TO BROWSER EXTENSION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Extension shows:
  âœ… POLICY_CREATED (69% confident)
  
  [More details...]
  - 3 fields modified
  - Form was empty before
  - Similar to 150+ CREATE events in training
```

---

## SLIDE 5: Simple Analogy

```
TRAINING IS LIKE LEARNING TO RECOGNIZE ANIMALS

Teacher shows you 1000 images:
  â€¢ Image 1: Photo of dog â†’ "This is DOG"
  â€¢ Image 2: Photo of cat â†’ "This is CAT"
  â€¢ Image 3: Photo of dog â†’ "This is DOG"
  ...

You notice patterns:
  "Dogs have: 4 legs, floppy ears, tail, barks"
  "Cats have: 4 legs, pointed ears, whiskers, meows"
  "Birds have: 2 legs, wings, beak, flies"

After seeing 1000 images 25 times each:
  You become expert at recognizing animals!


INFERENCE IS LIKE IDENTIFYING NEW ANIMALS

You see a NEW animal:
  "It has 4 legs, pointed ears, whiskers..."
  
You remember your training:
  "That matches CAT pattern!"
  
You say: "This is probably a CAT (90% sure)"

You're not learning anymore, just using learned patterns!


THE SAME CONCEPT IN ML:

TRAINING:
  Teacher: [1000 labeled config changes]
  Model: "I see patterns..."
  After 25 epochs: "I learned! CREATE = empty before"

INFERENCE:
  New config: "Empty before, 3 fields changed"
  Model: "I remember that pattern!"
  Output: "POLICY_CREATED (87% sure)"
```

---

## SLIDE 6: Key Metrics to Remember

```
TRAINING METRICS (How good is the model?)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Initial Loss:     47.32  (very bad, random)
Final Loss:       7.15   (very good, learned)
Improvement:      85% drop

Epochs:           25     (iterations)
Batches/epoch:    250    (samples grouped by 4)
Total updates:    6250   (25 Ã— 250)

Training time:    ~30 seconds on CPU
                  ~5 seconds on GPU


INFERENCE METRICS (How fast is prediction?)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Latency:          45-50 ms per prediction
  â€¢ Vectorization: 10ms
  â€¢ Network forward pass: 30ms
  â€¢ Softmax: 5ms

Accuracy:         85-92% (depends on data quality)
                  Improves with each retraining

Confidence:       Predictions have probability score
                  0.50 = uncertain (don't trust)
                  0.90 = confident (trust)
                  0.99 = very confident


MODEL SIZE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PyTorch weights:    1-3 MB (binary, heavy)
JSON export:        2-5 MB (text, larger)
Loaded in browser:  Uses ~5MB RAM

Parameters:         440,000 total
                    Tiny compared to GPT (7B+)

Why so small?
  â€¢ No deep layers (only 2 layers per MLP)
  â€¢ Limited feature dimensions (2000+847+200)
  â€¢ Simple architecture (MLPs, not Transformers)
```

---

## SLIDE 7: The Complete Journey

```
START HERE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User uses system   â”‚
â”‚  Makes config       â”‚
â”‚  changes            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ (1000+ changes collected)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DATA COLLECTION PHASE              â”‚
â”‚  â”œâ”€ Browser extension captures      â”‚
â”‚  â”œâ”€ Maps to canonical schema        â”‚
â”‚  â””â”€ Exports as JSON                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ (Upload to backend)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PREPROCESSING PHASE                â”‚
â”‚  â”œâ”€ Convert to tensors              â”‚
â”‚  â”œâ”€ Vectorize text (TF-IDF)         â”‚
â”‚  â”œâ”€ Flatten structures              â”‚
â”‚  â””â”€ Create batches                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRAINING PHASE (25 epochs)         â”‚
â”‚  â”œâ”€ Initialize random weights       â”‚
â”‚  â”œâ”€ Forward pass                    â”‚
â”‚  â”œâ”€ Calculate loss                  â”‚
â”‚  â”œâ”€ Backpropagation                 â”‚
â”‚  â”œâ”€ Update weights                  â”‚
â”‚  â””â”€ Loss: 47 â†’ 42 â†’ ... â†’ 7        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXPORT PHASE                       â”‚
â”‚  â”œâ”€ Extract learned weights         â”‚
â”‚  â”œâ”€ Convert PyTorch â†’ JSON          â”‚
â”‚  â””â”€ Compress (2-5 MB)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ (Deploy to browser)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INFERENCE PHASE (Daily)            â”‚
â”‚  â”œâ”€ User saves new config (trigger) â”‚
â”‚  â”œâ”€ Preprocess features             â”‚
â”‚  â”œâ”€ Forward pass (~50ms)            â”‚
â”‚  â””â”€ Return: Label + Confidence      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REPEAT MONTHLY                     â”‚
â”‚  â”œâ”€ Collect new examples            â”‚
â”‚  â”œâ”€ Retrain with larger dataset     â”‚
â”‚  â””â”€ Deploy improved model           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SLIDE 8: Quick Comparison

```
RULE-BASED (Old Way)          ML-BASED (New Way)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

How it works:
â””â”€ IF/ELSE hardcoded           â””â”€ Learned from 1000 examples

Accuracy:
â””â”€ ~60-70% (misses edge cases) â””â”€ 85-92% (learns patterns)

Scaling:
â””â”€ 500 lines per vendor        â””â”€ 1 model for all vendors

Updates:
â””â”€ Manual code changes          â””â”€ Retrain with new data

Speed:
â””â”€ Instant (simple regex)       â””â”€ 50ms (matrix math)

Learning:
â””â”€ Never improves               â””â”€ Better each month

Maintenance:
â””â”€ High (always fixing bugs)    â””â”€ Low (let data fix it)

Cost:
â””â”€ Dev-heavy (lots of coding)  â””â”€ Data-heavy (need examples)


WHEN TO USE WHICH?

Rule-Based GOOD FOR:
  âœ“ Simple patterns (hello = greeting)
  âœ“ 100% accuracy needed
  âœ“ Few vendors

ML GOOD FOR:
  âœ“ Complex patterns (config type detection)
  âœ“ Many vendors
  âœ“ Patterns that change over time
  âœ“ Scale to new vendors easily
```

---

## SLIDE 9: Troubleshooting

```
PROBLEM: Model always predicts "CREATED"

CAUSES & SOLUTIONS:
1ï¸âƒ£  Imbalanced data?
    Check: Count samples per class
    Fix: Collect more MODIFIED examples
    
2ï¸âƒ£  Wrong labels in training?
    Check: Manually verify 50 samples
    Fix: Relabel if needed
    
3ï¸âƒ£  Not enough training data?
    Check: Do you have 200+ samples per class?
    Fix: Collect 1000+ total samples
    
4ï¸âƒ£  Features not helpful?
    Check: Look at feature importance
    Fix: Add new features or improve collection
    
5ï¸âƒ£  Model underfitting (loss too high)?
    Check: Does loss drop each epoch?
    Fix: Train more epochs, bigger hidden layers


PROBLEM: Training is slow

CAUSES & SOLUTIONS:
1ï¸âƒ£  Large dataset?
    Check: How many samples? (>5000?)
    Fix: Sample randomly, use subset to test
    
2ï¸âƒ£  No GPU?
    Check: Is pytorch using CPU?
    Fix: Use GPU (RTX 2060+, ~$150)
    
3ï¸âƒ£  Inefficient code?
    Check: Use profiler
    Fix: Batch processing, vectorization


PROBLEM: Inference is slow (>200ms)

CAUSES & SOLUTIONS:
1ï¸âƒ£  Large model?
    Check: model_data.json size
    Fix: Reduce vocab size (2000 â†’ 1000)
    
2ï¸âƒ£  Complex vectorization?
    Check: Text preprocessing takes long
    Fix: Cache vectorizer, use simpler tokenizer
    
3ï¸âƒ£  Slow browser?
    Check: Mobile vs desktop
    Fix: Optimize for target device
```

---

## SLIDE 10: Future Improvements

```
ROADMAP FOR BETTER PREDICTIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PHASE 1 (Current):
  âœ… Text + Struct + Diff features
  âœ… 3-expert voting
  âœ… Single model for all vendors

PHASE 2 (Next Quarter):
  ğŸ”œ Add confidence thresholding
  ğŸ”œ Reject predictions <70% confidence
  ğŸ”œ Collect and store uncertain cases
  ğŸ”œ Monthly retraining pipeline

PHASE 3 (6 Months):
  ğŸ”œ Per-vendor fine-tuning
  ğŸ”œ Transfer learning (faster retraining)
  ğŸ”œ Multi-label classification (multiple tags)
  ğŸ”œ Uncertainty quantification

PHASE 4 (1 Year):
  ğŸ”œ Anomaly detection (unusual changes)
  ğŸ”œ Risk scoring (is this risky?)
  ğŸ”œ Sequence modeling (time-series)
  ğŸ”œ Ensemble multiple models


WHAT WOULD IMPROVE ACCURACY MOST?

1. More data (highest impact)
   â””â”€ 1000 â†’ 5000 samples = +15% accuracy
   
2. Better labeling (high impact)
   â””â”€ Clean labels = +10% accuracy
   
3. Feature engineering (medium impact)
   â””â”€ Add new features = +5% accuracy
   
4. Bigger model (low impact)
   â””â”€ 128 â†’ 256 hidden = +2% accuracy
```

---

