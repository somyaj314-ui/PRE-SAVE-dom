# ğŸ¤– TRAINING & INFERENCE: DETAILED WALKTHROUGH

Complete explanation of how the model learns and makes predictions.

---

## ğŸ“š PART 1: WHAT IS TRAINING?

### **Simple Definition**
Training is when the AI learns from 1000+ **labeled examples** and adjusts its internal weights to recognize patterns.

```
ANALOGY: Learning to Drive
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Training Phase:
  1. You see 1000 driving situations (examples)
  2. Instructor tells you correct action (label)
  3. You practice, make mistakes, learn
  4. After 25 practice sessions, you know patterns
  
Our ML Model:
  1. Sees 1000 config changes (examples)
  2. Knows if it's CREATE or MODIFY (label)
  3. Adjusts weights through backpropagation
  4. After 25 epochs, recognizes patterns
```

---

## ğŸ”§ PART 2: THE TRAINING PIPELINE

### **Step 1: Prepare Training Data**

**Input**: `train.pkl` (preprocessed data)

```python
# What's inside train.pkl
{
  "texts": [        # Text field content
    "policy Allow HTTP port1 port2",
    "policy Allow DNS port3 port4",
    ...  (1000 documents)
  ],
  
  "tfidf_vectors": [  # Already vectorized (2000 dims each)
    [0.12, 0.0, 0.45, ..., 0.02],
    [0.08, 0.03, 0.22, ..., 0.15],
    ...  (1000 vectors)
  ],
  
  "structs": [      # Binary field presence (847 dims each)
    [1, 0, 1, 1, 0, 1, ...],
    [1, 1, 1, 0, 1, 0, ...],
    ...  (1000 vectors)
  ],
  
  "diffs": [        # Which fields changed (200 dims each)
    [0, 1, 0, 1, 0, ...],
    [1, 0, 1, 0, 0, ...],
    ...  (1000 vectors)
  ],
  
  "labels": [       # Ground truth (what actually happened)
    "POLICY_CREATED",
    "POLICY_MODIFIED",
    "INTERFACE_CREATED",
    ...  (1000 labels)
  ]
}
```

### **Step 2: Create Batches**

```python
from torch.utils.data import DataLoader

# DataLoader batches the data
# batch_size = 4 means: process 4 samples at a time

DataLoader(dataset, batch_size=4, shuffle=True)

EPOCH 1, BATCH 1:
  â”œâ”€ Sample 0: text[0], struct[0], diff[0], label[0]
  â”œâ”€ Sample 1: text[1], struct[1], diff[1], label[1]
  â”œâ”€ Sample 2: text[2], struct[2], diff[2], label[2]
  â””â”€ Sample 3: text[3], struct[3], diff[3], label[3]
              â†“
       Process together
              â†“
       Calculate loss
              â†“
       Update weights

EPOCH 1, BATCH 2:
  â”œâ”€ Sample 4: ...
  â”œâ”€ Sample 5: ...
  â”œâ”€ Sample 6: ...
  â””â”€ Sample 7: ...
```

---

## ğŸ§  PART 3: THE NEURAL NETWORK ARCHITECTURE

### **Model Structure**

```
TRIPLE MLP FUSION ARCHITECTURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INPUT FEATURES (from 1 training sample):
  â”œâ”€ text_vector: [2000] TF-IDF
  â”œâ”€ struct_vector: [847] binary
  â””â”€ diff_vector: [200] binary


PROCESSING (3 Parallel Paths):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PATH 1: TEXT MLP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
text[2000]
   â†“
Linear(2000 â†’ 128)  [Learn text patterns]
   â†“ (2000*128 = 256K weights)
Hidden[128]
   â†“
ReLU (Activate non-linear patterns)
   â†“
Linear(128 â†’ 128)   [Refine patterns]
   â†“ (128*128 = 16K weights)
text_output[128]


PATH 2: STRUCT MLP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
struct[847]
   â†“
Linear(847 â†’ 128)   [Learn form complexity]
   â†“ (847*128 = 108K weights)
Hidden[128]
   â†“
ReLU
   â†“
Linear(128 â†’ 128)   [Refine]
   â†“
struct_output[128]


PATH 3: DIFF MLP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
diff[200]
   â†“
Linear(200 â†’ 128)   [Learn change patterns]
   â†“ (200*128 = 25K weights)
Hidden[128]
   â†“
ReLU
   â†“
Linear(128 â†’ 128)   [Refine]
   â†“
diff_output[128]


FUSION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Concatenate: [128] + [128] + [128] = [384]
   â†“
Final Classifier
Linear(384 â†’ num_classes)
   â†“ (384 * num_classes weights)
logits[5]   (example: 5 change types)
   â†“
Softmax (Convert to probabilities)
   â†“
probs[5]    [0.85, 0.10, 0.03, 0.01, 0.01]
             POLICY_CREATED is 85% likely
```

### **Total Parameters**

```
Weights:
  Text MLP:      256K + 16K = 272K
  Struct MLP:    108K + 16K = 124K
  Diff MLP:      25K + 16K = 41K
  Final FC:      384 * num_classes â‰ˆ 2K
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL:         ~440K parameters

For comparison:
  Large language models: 7B - 70B parameters
  Your model: Ultra-lightweight for browser
```

---

## ğŸ“Š PART 4: TRAINING LOOP (Epoch by Epoch)

### **What Happens in One Epoch**

```
EPOCH 1 (Processing all 1000 samples)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Initialize: Random weights for all MLPs

FOR EACH BATCH (250 batches total, 4 samples per batch):
  â”‚
  â”œâ”€ BATCH 1 (Samples 0-3):
  â”‚   â”‚
  â”‚   â”œâ”€ FORWARD PASS (Model makes predictions)
  â”‚   â”‚   â”œâ”€ text_mlp processes text[2000]
  â”‚   â”‚   â”œâ”€ struct_mlp processes struct[847]
  â”‚   â”‚   â”œâ”€ diff_mlp processes diff[200]
  â”‚   â”‚   â”œâ”€ Concatenate outputs [384]
  â”‚   â”‚   â”œâ”€ Final FC layer [num_classes]
  â”‚   â”‚   â””â”€ Softmax â†’ probabilities
  â”‚   â”‚
  â”‚   â”‚   Result: 4 predictions (one per sample)
  â”‚   â”‚   Example:
  â”‚   â”‚   â”œâ”€ Sample 0: [0.85, 0.10, 0.03, 0.01, 0.01]
  â”‚   â”‚   â”œâ”€ Sample 1: [0.02, 0.92, 0.03, 0.02, 0.01]
  â”‚   â”‚   â”œâ”€ Sample 2: [0.15, 0.10, 0.70, 0.03, 0.02]
  â”‚   â”‚   â””â”€ Sample 3: [0.05, 0.05, 0.05, 0.80, 0.05]
  â”‚   â”‚
  â”‚   â”œâ”€ CALCULATE LOSS (How wrong are we?)
  â”‚   â”‚   True labels: ["CREATED", "MODIFIED", "OTHER", "OTHER"]
  â”‚   â”‚   
  â”‚   â”‚   Cross-Entropy Loss:
  â”‚   â”‚   Sample 0: predicted 0.85 for CREATED âœ“ (correct, low loss)
  â”‚   â”‚   Sample 1: predicted 0.92 for MODIFIED âœ“ (correct, low loss)
  â”‚   â”‚   Sample 2: predicted 0.70 for OTHER âœ“ (correct, low loss)
  â”‚   â”‚   Sample 3: predicted 0.80 for OTHER âœ“ (correct, low loss)
  â”‚   â”‚   
  â”‚   â”‚   Batch Loss = average of all 4 losses = 0.15
  â”‚   â”‚
  â”‚   â”œâ”€ BACKWARD PASS (Calculate gradients)
  â”‚   â”‚   For each weight w:
  â”‚   â”‚   gradient = âˆ‚Loss / âˆ‚w
  â”‚   â”‚   
  â”‚   â”‚   Shows: "This weight needs to increase/decrease
  â”‚   â”‚            to reduce loss next time"
  â”‚   â”‚
  â”‚   â””â”€ UPDATE WEIGHTS (Optimizer step)
  â”‚       Adam optimizer updates all weights:
  â”‚       w_new = w_old - learning_rate * gradient
  â”‚       
  â”‚       learning_rate = 0.001 (small steps to avoid overfitting)
  â”‚
  â”œâ”€ BATCH 2 (Samples 4-7): Repeat...
  â”œâ”€ BATCH 3 (Samples 8-11): Repeat...
  â”œâ”€ ...
  â””â”€ BATCH 250 (Samples 996-999): Repeat...

After EPOCH 1:
  Total Loss = sum of all batch losses = 47.32
  
  Console Output:
  "Epoch 1, Loss: 47.3200"
```

### **What Happens Across 25 Epochs**

```
EPOCH 1: Loss = 47.32  (Model is very confused)
EPOCH 2: Loss = 42.15  (Getting better)
EPOCH 3: Loss = 38.87  (Patterns emerging)
EPOCH 4: Loss = 35.22  (Weights adjusting)
EPOCH 5: Loss = 31.65  (Starting to converge)
...
EPOCH 20: Loss = 8.45   (Much better!)
EPOCH 21: Loss = 7.98   (Learning rate decreasing)
EPOCH 22: Loss = 7.65
EPOCH 23: Loss = 7.43
EPOCH 24: Loss = 7.28
EPOCH 25: Loss = 7.15   (Final loss)

The model has learned! Loss dropped 85% (47 â†’ 7)
This means: weights now correctly predict labels
```

### **The Math Behind It (Simple)**

```
Loss Function = CrossEntropyLoss
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For each prediction:
  True label:       POLICY_CREATED (index 0)
  Predicted probs:  [0.85, 0.10, 0.03, 0.01, 0.01]
  
  Loss = -log(0.85) = 0.163
  (If prediction was [0.5, ...], loss = -log(0.5) = 0.693)
  (If prediction was [0.1, ...], loss = -log(0.1) = 2.303)
  
  Lower probability for correct class = HIGHER loss

Backpropagation = Computing gradients
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For weight w in text_mlp.l1:
  âˆ‚Loss/âˆ‚w tells us:
  - If positive: increase w slightly (loss will decrease)
  - If negative: decrease w slightly (loss will decrease)
  - If zero: weight doesn't affect loss (no change needed)

Adam Optimizer = Smart weight updater
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Keeps track of:
  1. Current gradient (how to adjust now)
  2. Momentum (how we adjusted before)
  3. Adaptive learning rate (speeds up important weights)

Updates weights more intelligently than simple SGD
```

---

## âš¡ PART 5: SAVING THE TRAINED MODEL

### **What Gets Saved**

```python
# After 25 epochs, save to disk

model_artifacts = {
  "model_state_dict": {
    # All the LEARNED weights
    "txt_mlp.m.0.weight": [[...]...],  # 2000x128
    "txt_mlp.m.0.bias": [...],         # 128
    "txt_mlp.m.2.weight": [[...]...],  # 128x128
    "txt_mlp.m.2.bias": [...],         # 128
    
    "struct_mlp.m.0.weight": [...],    # 847x128
    "struct_mlp.m.0.bias": [...],      # 128
    "struct_mlp.m.2.weight": [...],    # 128x128
    "struct_mlp.m.2.bias": [...],      # 128
    
    "diff_mlp.m.0.weight": [...],      # 200x128
    "diff_mlp.m.0.bias": [...],        # 128
    "diff_mlp.m.2.weight": [...],      # 128x128
    "diff_mlp.m.2.bias": [...],        # 128
    
    "fc.weight": [...],                # num_classes x 384
    "fc.bias": [...]                   # num_classes
  },
  
  "vectorizer": <TfidfVectorizer>,  # Learned vocabulary
  "label_map": {                     # String to index mapping
    "POLICY_CREATED": 0,
    "POLICY_MODIFIED": 1,
    "INTERFACE_CREATED": 2,
    ...
  },
  "struct_dim": 847,
  "diff_dim": 200
}

pickle.dump(model_artifacts, open("model_artifacts.pkl", "wb"))
```

---

## ğŸ”„ PART 6: CONVERTING TO JAVASCRIPT

### **Why Convert to JSON?**

```
Python Model (PyTorch):            JavaScript Model (JSON):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Can't run in browser                Runs directly in browser
Binary format (not portable)         Text format (universal)
Dependencies (pytorch, sklearn)      No dependencies needed
500 MB file size                     2-5 MB file size
```

### **The Export Process**

```python
# export_model.py

# Step 1: Extract TF-IDF
vocab = vectorizer.vocabulary_  # {word: index, ...}
idf = vectorizer.idf_          # [1.2, 0.8, ...]

# Step 2: Extract weights (convert to lists)
model_data = {
  "tfidf": {
    "vocab": ["policy", "allow", "port1", ...],  # 2000 words
    "idf": [1.2, 0.8, 0.95, ...]                 # 2000 weights
  },
  
  "model": {
    "txt_mlp": {
      "l1_weight": [[...], [...], ...],     # 128 rows Ã— 2000 cols
      "l1_bias": [...],                     # 128 values
      "l2_weight": [[...], [...], ...],     # 128 rows Ã— 128 cols
      "l2_bias": [...]                      # 128 values
    },
    "struct_mlp": {...},
    "diff_mlp": {...},
    "fc": {
      "weight": [[...], [...], ...],        # num_classes rows Ã— 384 cols
      "bias": [...]                         # num_classes values
    }
  },
  
  "metadata": {
    "labels": {0: "POLICY_CREATED", 1: "MODIFIED", ...},
    "struct_dim": 847,
    "diff_dim": 200
  }
}

# Step 3: Save as JSON
json.dump(model_data, open("model_data.json", "w"))
# File size: 2-5 MB
```

---

## ğŸ¯ PART 7: LIVE PREDICTION (INFERENCE)

### **How Predictions Work in Browser**

```
NEW FORM SUBMISSION (User saves a config)
         â”‚
         â–¼
   ml-inference.js
   â”œâ”€ Preprocess features (same as training)
   â”œâ”€ Run through model
   â””â”€ Return prediction + confidence


STEP-BY-STEP PREDICTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INPUT: New config change
{
  "metadata": {
    "vendor": "fortigate",
    "object_type": "policy"
  },
  "data": {
    "before": {},  (empty = CREATE)
    "after": {
      "name": "Allow HTTPS",
      "source_interface": "port1",
      "destination_interface": "port3",
      ...
    }
  },
  "changes": [
    {"field": "name", "old": "", "new": "Allow HTTPS"},
    {"field": "source_interface", "old": "", "new": "port1"},
    ...
  ]
}

STEP 1: TEXT FEATURE VECTORIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Extract text:
  "name Allow HTTPS source_interface port1 destination_interface port3 ..."

Tokenize:
  ["name", "allow", "https", "source_interface", "port1", ...]

Look up in vocab (from TF-IDF):
  "name" â†’ index 5
  "allow" â†’ index 12
  "https" â†’ index 203
  "source_interface" â†’ index 45
  ...

Count occurrences & apply IDF:
  tf[5] = 1 Ã— idf[5] = 1.2
  tf[12] = 1 Ã— idf[12] = 0.8
  tf[203] = 1 Ã— idf[203] = 1.1
  tf[45] = 1 Ã— idf[45] = 0.95
  ...rest = 0

Normalize (L2):
  All values / sqrt(sum of squares)
  
  Result: [0.05, 0.0, ..., 0.08, ..., 0.04, ...]  (2000 dims)


STEP 2: STRUCTURAL FEATURE VECTORIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Get feature keys:
  ["name", "source_interface", "destination_interface", ..., 847 total]

For each key, check if it has a value in "after":
  "name": "Allow HTTPS" â†’ 1 (has value)
  "source_interface": "port1" â†’ 1
  "destination_interface": "port3" â†’ 1
  "log_traffic": undefined â†’ 0 (missing)
  ...
  
  Result: [1, 1, 1, 0, 1, ..., 0]  (847 dims)


STEP 3: DIFF FEATURE VECTORIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Get changed field names:
  ["name", "source_interface", "destination_interface"]

Hash each field:
  hash("name") = 12345 % 200 = 45
  hash("source_interface") = 67890 % 200 = 78
  hash("destination_interface") = 54321 % 200 = 21
  
  Set bits at those positions:
  diff_vec = [0, 0, ..., 1 (at 21), ..., 1 (at 45), ..., 1 (at 78), ...]
  
  Result: [0, 1, 0, ..., 1, ..., 1, 0, ...]  (200 dims)


STEP 4: FORWARD PASS (Neural Network)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PATH 1: Text through txt_mlp
  input: [0.05, 0.0, ..., 0.08] (2000)
  â”œâ”€ Linear layer: multiply by weights + add bias
  â”‚  result: [2.3, -0.5, 1.2, ..., 0.8]  (128)
  â”œâ”€ ReLU: max(0, x) for each value
  â”‚  result: [2.3, 0.0, 1.2, ..., 0.8]   (128)
  â””â”€ Linear layer again
     output: [1.5, 0.3, ..., -0.2]        (128)

PATH 2: Struct through struct_mlp
  input: [1, 1, 1, 0, 1, ...] (847)
  â””â”€ Same process
     output: [0.8, 1.1, 0.2, ..., 0.5]   (128)

PATH 3: Diff through diff_mlp
  input: [0, 1, 0, ..., 1] (200)
  â””â”€ Same process
     output: [2.1, -0.3, 0.9, ..., 0.1]  (128)

Concatenate all three:
  [1.5, 0.3, ..., -0.2, 0.8, 1.1, 0.2, ..., 0.5, 2.1, -0.3, 0.9, ..., 0.1]
  = [384 values]

Final classification:
  Multiply by FC weight matrix + add bias:
  logits = [2.3, -1.5, 0.8, 0.2, -0.3]  (5 classes)


STEP 5: SOFTMAX (Convert to Probabilities)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logits = [2.3, -1.5, 0.8, 0.2, -0.3]

exp each value:
  exp(2.3) = 9.97
  exp(-1.5) = 0.22
  exp(0.8) = 2.23
  exp(0.2) = 1.22
  exp(-0.3) = 0.74

Normalize (divide by sum):
  sum = 9.97 + 0.22 + 2.23 + 1.22 + 0.74 = 14.38
  
  probs = [
    9.97/14.38 = 0.693  (69.3%)
    0.22/14.38 = 0.015  (1.5%)
    2.23/14.38 = 0.155  (15.5%)
    1.22/14.38 = 0.085  (8.5%)
    0.74/14.38 = 0.051  (5.1%)
  ]
  
  Sum = 1.0 (probabilities)


STEP 6: FINAL PREDICTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Find highest probability:
  Max = 0.693 at index 0
  
  label_map = {
    0: "POLICY_CREATED",
    1: "POLICY_MODIFIED",
    2: "INTERFACE_CREATED",
    3: "ADMIN_USER_CREATED",
    4: "OTHER"
  }
  
  PREDICTION = label_map[0] = "POLICY_CREATED"
  CONFIDENCE = 0.693 = 69.3%
  
  
RETURN to Browser Extension
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{
  "label": "POLICY_CREATED",
  "confidence": 0.693,
  "probabilities": [0.693, 0.015, 0.155, 0.085, 0.051]
}

â±ï¸ Time: ~50ms (on modern browser)
```

---

## ğŸ“ˆ COMPLETE FLOW: Training â†’ Prediction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRAINING PHASE (Backend, One-Time)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  1. Collect 1000+ config changes (data collection)    â”‚
â”‚  2. Preprocess to tensors (preprocessing.py)          â”‚
â”‚  3. Train neural network (train.py)                   â”‚
â”‚     â”œâ”€ 25 epochs                                      â”‚
â”‚     â”œâ”€ 250 batches per epoch                          â”‚
â”‚     â””â”€ Update 440K weights                            â”‚
â”‚  4. Save model (model_artifacts.pkl)                  â”‚
â”‚  5. Export to JavaScript (export_model.py)            â”‚
â”‚     â””â”€ model_data.json (2-5 MB)                       â”‚
â”‚  6. Deploy to browser extension                       â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    REPEAT MONTHLY
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INFERENCE PHASE (Browser, Real-Time)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  When user saves a config:                            â”‚
â”‚  1. Capture features (same as training)               â”‚
â”‚     â”œâ”€ Text vectorization (TF-IDF)                    â”‚
â”‚     â”œâ”€ Struct vectorization (binary)                  â”‚
â”‚     â””â”€ Diff vectorization (hashed)                    â”‚
â”‚  2. Load model_data.json (cached)                     â”‚
â”‚  3. Forward pass through MLPs                         â”‚
â”‚  4. Softmax â†’ probabilities                           â”‚
â”‚  5. Return prediction + confidence                    â”‚
â”‚                                                        â”‚
â”‚  Latency: ~50ms                                       â”‚
â”‚  Accuracy: Improves with each retraining              â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ KEY CONCEPTS SUMMARY

### **Training Concepts**

| Term | Meaning | Example |
|------|---------|---------|
| **Epoch** | One pass through all data | 25 epochs = see all 1000 samples 25 times |
| **Batch** | Small subset of data | batch_size=4 = 4 samples at a time |
| **Loss** | How wrong the model is | Loss=47 (bad), Loss=7 (good) |
| **Backprop** | Computing gradients | Shows which weights to adjust |
| **Gradient** | Direction to adjust weight | Positive = increase weight |
| **Learning Rate** | Step size for updates | 0.001 = small careful steps |

### **Inference Concepts**

| Term | Meaning | Example |
|------|---------|---------|
| **Forward Pass** | Data â†’ Predictions | Input [2000+847+200] â†’ Output [5] |
| **Vectorization** | Convert text to numbers | "policy" â†’ [0.12, 0.0, ...] |
| **Softmax** | Convert scores to probabilities | logits â†’ sum to 1.0 |
| **Confidence** | Probability of prediction | 69.3% = fairly sure |
| **Latency** | Time to predict | ~50ms per prediction |

---

## ğŸ’¾ FILES AT EACH STAGE

```
Training:
  universal_training_data_*.json     (1000+ samples, 2-10 MB)
  â””â”€ preprocessing.py
     train.pkl                       (tensors, 10-50 MB)
     â””â”€ train.py
        model_artifacts.pkl          (trained weights, 1-3 MB)
        â””â”€ export_model.py
           model_data.json           (JavaScript model, 2-5 MB)

Inference:
  model_data.json                    (loaded in browser cache)
  â””â”€ ml-inference.js
     â””â”€ Real-time predictions
```

---

## ğŸ” MONITORING TRAINING

### **What to Watch For**

```
âœ… GOOD SIGNS:
   â€¢ Loss decreases each epoch (47 â†’ 42 â†’ 38 â†’ ...)
   â€¢ Loss curve smooth, not bouncy
   â€¢ No NaN (Not a Number) errors
   â€¢ Training completes in <1 minute

âŒ BAD SIGNS:
   â€¢ Loss increases or stays flat
   â€¢ Loss jumps around wildly
   â€¢ NaN or Inf values
   â€¢ Memory error / out of VRAM
   
â†”ï¸ NORMAL SIGNS:
   â€¢ Loss decreases slower after epoch 10
   â€¢ Loss plateaus around epoch 20
   â€¢ Learning rate needs tuning (adjust from 1e-3)
```

---

## ğŸš€ IMPROVING MODEL ACCURACY

### **If predictions are inaccurate:**

```
Problem: Model always predicts POLICY_CREATED

Solutions:
1. Imbalanced data?
   â€¢ Count samples per class
   â€¢ Should be ~200-300 per class
   
2. Not enough data?
   â€¢ Collect 1000+ samples minimum
   â€¢ Each class needs 200+ samples
   
3. Bad features?
   â€¢ Check if features are meaningful
   â€¢ Visualize which features matter
   
4. Wrong labels?
   â€¢ Verify training data labels are correct
   â€¢ Relabel if needed
   
5. Model too simple?
   â€¢ Increase hidden layer size (128 â†’ 256)
   â€¢ Add more MLP layers
   
6. Training too few epochs?
   â€¢ Run 50 epochs instead of 25
   â€¢ Monitor when loss plateaus
```

---

